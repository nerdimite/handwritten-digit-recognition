{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22796d1e-d91c-466e-8ccf-6a74715bd2d0",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition\n",
    "## Deep Learning (18CSE484T) Group Project\n",
    "\n",
    "### Group Members:\n",
    "\n",
    "- Bhavesh Laddagiri (RA1911026030032)\n",
    "- Akshaj Vishwanathan (RA1911026030003)\n",
    "- Hardik Gupta (RA1911026030027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f25e894-d2da-4835-9983-b8625423b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb3358-555b-419b-b0ff-ab42b376adc6",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64b53e9-bfd7-46da-9974-99e1d3b5494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7a3def-abe1-436f-bf0b-1862c40b03f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download train and test data\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3969144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a84c72-68c6-4894-9b38-8d2ffb35d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([96, 1, 28, 28])\n",
      "Shape of y:  torch.Size([96]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 96\n",
    "\n",
    "# Create data loaders.\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560983d-8a8c-4f18-b0ed-b6d6d4e03bc2",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58abc580-913c-40a2-9859-4970c15786b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device automatically for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f597a98f-07f4-4d6c-a0a9-20b84aa72431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 3),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 16, 3),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        \n",
    "        # print(self.calculate_size())\n",
    "        self.fc = nn.Sequential(nn.Linear(self.calculate_size(), 256),\n",
    "                                nn.Linear(256, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape should be [batch_size, channels, height, width]\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # reshape the output to flat row vectors of length batch_size\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def calculate_size(self):\n",
    "        '''Utility to calculate the output size \n",
    "        from the convolution layers'''\n",
    "        x = torch.randn((1, 1, 28, 28))\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        size = x.flatten().shape[0]\n",
    "        return size\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a057a8-68ab-4009-8112-73567f3517cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0638,  0.0525, -0.0563,  0.0391, -0.0057, -0.0267,  0.0323, -0.1234,\n",
       "         -0.0034,  0.0641]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(train_data[0][0].unsqueeze(0).to(device))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41a6a1-c080-43f4-9d05-44f46af00934",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77d4752-923a-4a45-98f3-26c73ff9c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f06f8d9e-9859-42a5-9c1f-36817ab8c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move the inputs to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Model forward pass\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Train Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\\r\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e272c0a9-bde4-42af-b40a-af228c8b6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # Move the inputs to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Model forward pass\n",
    "            pred = model(X)\n",
    "            \n",
    "            # Compute the loss and add it to the test_loss variable\n",
    "            test_loss += criterion(pred, y).item()\n",
    "            \n",
    "            # Calculate the number of correct predictions and add it to correct variable\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    # Calculate Mean loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Results: \\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520dafa1-e3ba-4f55-be69-4f4406c817e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Loss: 0.060411  [57600/60000]\n",
      "Validation Results: \n",
      "Accuracy: 97.3%, Avg loss: 0.083398\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Loss: 0.008984  [57600/60000]\n",
      "Validation Results: \n",
      "Accuracy: 98.0%, Avg loss: 0.064336\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Loss: 0.152270  [57600/60000]\n",
      "Validation Results: \n",
      "Accuracy: 98.0%, Avg loss: 0.064719\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)\n",
    "    test(test_loader, model, criterion)\n",
    "    print()\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ca16314-5783-4d91-b80b-b44424170d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to mnist_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "path = \"mnist_model.pth\"\n",
    "torch.save(model.state_dict(), path)\n",
    "print(f\"Saved PyTorch Model State to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e09c6-001e-4621-a584-1ab2b558e5fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb41fc9-aab3-4991-914d-826d5a19c713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload Model using the saved weights\n",
    "model = CNNModel()\n",
    "model.load_state_dict(torch.load(path, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e4a4216-01ca-438a-9829-452662271c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    # Preprocess\n",
    "    img_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.inference_mode():\n",
    "        logits = model(img_tensor)\n",
    "        probs = logits.softmax(dim=-1).squeeze()\n",
    "    \n",
    "    # Postprocessing\n",
    "    output = []\n",
    "    for i, prob in enumerate(probs):\n",
    "        output.append((i, round(prob.item() * 100, 2)))\n",
    "    \n",
    "    return sorted(output, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7dd4b0-cbed-41da-aa46-dc9fb2cc378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80dfc436-e5ae-4a4f-9b79-3e8835ef4608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100 at 0x182801EC3D0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a random image from test set\n",
    "idx = torch.randint(0, len(test_data), size=(1,))[0]\n",
    "test_image, test_label = test_data[idx]\n",
    "test_image = to_pil(test_image)\n",
    "\n",
    "# View image\n",
    "test_image.resize((100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fdc36ff-b66d-4e8a-811e-9734e5152fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted as 7 with 100.0% Confidence\n",
      "Probabilities: [(7, 100.0), (0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (8, 0.0), (9, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(test_image)\n",
    "print(f'Predicted as {preds[0][0]} with {preds[0][1]}% Confidence')\n",
    "print('Probabilities:', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1b2cd-7a30-47eb-8570-2cef80aae8f0",
   "metadata": {},
   "source": [
    "## Prepare for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af84fc47-7cfc-429c-9cfe-31a6a3793736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model to torchscript using JIT trace\n",
    "jit_model = torch.jit.trace(model, torch.randn(1, 1, 28, 28), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d236ce-0470-4420-8a40-c9b429511843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the traced computation graph\n",
    "jit_model.save('model_jit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ed516-719c-4ab1-911b-b5ae476571bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60fbf1aecf0122793952a73a80d27bc8732eff9e143c13520ca117508929b1c7"
  },
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
